# Results and Insights from the First Rounds Played

After the round from January 19th, 2026 gained a bit more attention than previous rounds, we wanted to take a closer look at the results.

## The Results

### Video Mode

Video mode was played almost 7,000 times in the first week, while photo mode was played just over 3,500 times. Many players commented on Reddit that they found video mode overall quite easy and could easily answer all 5 rounds correctly. But do these results actually reflect in the statistics?

After 7,000 games, the 5 images were correctly identified with the following probabilities:

- 64.12%
- 61.95%
- 73.49%
- 88.97%
- 84.45%

Overall, people were mostly able to identify which of the two videos was artificially generated, depending on the video. Interestingly, the first two images were only slightly above 60%, while players better identified the later videos. So even within a single round, a learning effect was already evident.

However, the numbers confirm the suspicion that on Reddit - and especially in the communities where I promoted the game - there are technically savvy people who have already engaged more with this topic. Additionally, those who performed well are probably more likely to comment.

### Photo Mode

The following results emerged for photo mode:

- 78.18%
- 36.62%
- 79.17%
- 82.61%
- 54.61%

As expected, it's significantly harder to recognize whether an image was generated by AI or not. Particularly interesting is that for this image, well below 50% guessed correctly:

![Scandinavia](/images/skandinavien_sample.png)

The correct answer in this case would have been B.

## Insights

Many Redditors report that when guessing, they're misled by which images or videos look "too perfect." One of the major challenges for the game's difficulty is finding good original images. Especially in the videos from the database I use, there are many examples that don't have very high resolution or were captured in an "amateurish" way. The AI results suggest that cinema-quality films were predominantly used for training.

The AI also struggled greatly to replicate unstable camera movements well, even though these were actually always included in the prompt. The cameras in the generated videos were almost always completely static or had very stabilized and smooth camera movements, while the originals sometimes had small jolts or instabilities in the camera work.

I'm excited to see how things develop in the coming weeks. Thanks to everyone who participated!
